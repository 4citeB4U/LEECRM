import re

class AgenticController:
    def __init__(self, max_chunk_size=1000):
        self.memory = {
            'chunking_strategy': self.default_chunking,
            'max_chunk_size': max_chunk_size,
            'task_plan': [],
            'processed_chunks': [],
            'results': []
        }

    def default_chunking(self, text, max_size):
        # Split text into sentences safely under max_size limit
        sentences = re.split(r'(?<=[.!?])\s+', text)
        chunks = []
        current_chunk = ""

        for sentence in sentences:
            if len(current_chunk) + len(sentence) + 1 <= max_size:
                current_chunk += (" " if current_chunk else "") + sentence
            else:
                chunks.append(current_chunk)
                current_chunk = sentence
        if current_chunk:
            chunks.append(current_chunk)
        return chunks

    def preprocess_input(self, input_text):
        # Check if input fits, else chunk
        if len(input_text) <= self.memory['max_chunk_size']:
            return [input_text]
        return self.memory['chunking_strategy'](input_text, self.memory['max_chunk_size'])

    def plan_tasks(self, chunks):
        # Plan a subtask per chunk
        self.memory['task_plan'] = [f"Process chunk {i+1} of {len(chunks)}" for i in range(len(chunks))]

    def execute(self, input_text, call_tool):
        # call_tool(chunk) = actual tool call function passed externally
        chunks = self.preprocess_input(input_text)
        self.plan_tasks(chunks)

        for idx, chunk in enumerate(chunks):
            if len(chunk) > self.memory['max_chunk_size']:
                raise ValueError(f"Chunk {idx+1} too large for tool: {len(chunk)} chars")

            # Optional: You can pass partial results or context here if your tool supports
            result = call_tool(chunk)
            self.memory['processed_chunks'].append(chunk)
            self.memory['results'].append(result)

            # Update memory state, could add dynamic chunking or retry logic here

        return self.aggregate_results()

    def aggregate_results(self):
        # Combine chunk results coherently, could add smarter merging here
        return "\n".join(self.memory['results'])


Hereâ€™s how it works step-by-step:

Check input size: If the input is already small enough, it sends it as is.

Chunking: If the input is too large, it splits the text into sentences, then groups these sentences into smaller chunks that each fit within the size limit.

Task planning: It plans to process each chunk individually.

Tool calls: It sends each chunk separately to the tool, ensuring no chunk is too big.

Memory: It keeps track of all chunks processed and their results.

Aggregation: After processing all chunks, it combines the results into one final output.
